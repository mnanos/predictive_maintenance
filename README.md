# predictive_maintenance
# Predictive Maintenance AI framework
## 1. greate DataSet from .wav files
python3 Create_Dataset.py -dataset-params "{ Wav_folder : c:\ML1\ML_pipeline_code_multiple_classes_v1\tst_dataset , Target_folder : c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst , Fs : 44100 , seq_dur : 5 , classes_lookup : { Moter_single : 0 , Motor_gran_no_chain : 1 , seatrak_all_elements : 2 , kinhsh_koble : 3 } , FE_params : { front_end_name : STFT_custom , a : 768 , M : 1024 , support : 1024 } , preproc : None } "

## 2. train model
python3 train.py --root c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst --nb_classes 4 --output c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst\pretr_model --epochs 10

## 3. evaluate
python3 evaluate.py --method-name TST --Model_dir c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst/pretr_model --root_TEST_dir c:\ML1\ML_pipeline_code_multiple_classes_v1\tst_dataset --evaldir c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst\evaldir

## 4. inference
python3 inference.py --Model_dir c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst\pretr_model --input-wav c:\ML1\ML_pipeline_code_multiple_classes_v1\tst_dataset\test\kinhsh_koble\κίνηση-κομπλε-3.wav  

## 5. inference real time
python3 inference_real_time.py --Model_dir c:\ML1\ML_pipeline_code_multiple_classes_v1\Spectrograms_tst\pretr_model


## 0) in order to install all the libraries we want:
pip install -r requirements.txt


## To run an experiment, follow these commands in sequence:
--------------------------------------------------------

/home/user/ML_pipeline_code_multiple_classes_v0/ instead of  /home/mnanos/ML_pipeline_code_multiple_classes_v0/

### 1) DATA, Create Dataset 
-----------------------------------------------------------------------------------------
COMMAND:
python3 Create_Dataset.py -dataset-params "{ Wav_folder : /home/mnanos/ML_pipeline_code_multiple_classes_v0/tst_dataset , Target_folder : /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst , Fs : 14700 , seq_dur : 5 , classes_lookup : { Moter_single : 0 , Motor_gran_no_chain : 1 , seatrak_all_elements : 2 , kinhsh_koble : 3 } , FE_params : { front_end_name : STFT_custom , a : 768 , M : 1024 , support : 1024 } , preproc : None } "

Parameter explaination:
    Wav_folder-> (STR) 	This is the PATH of the directory, which must have the following structure; the files are from Audacity:

   
   Wav_folder
	 ├── train
	 │    └── ith_class_wav  └── all .wav files you consider to belong to the i-th class
	 │
	 ├── valid
	 │    └── ith_class_wav  └── all .wav files you consider to belong to the i-th class
	 │
	 └── test
		  └── ith_class_wav  └── all .wav files you consider to belong to the i-th class

Target_folder → (STR) The path to the directory (created if it does not already exist) where the following files will be saved:

- Spec_seg_pair_list_train.pt → An iterable variable containing the training example pairs (In_spectr, label)
- Spec_seg_pair_list_valid.pt → An iterable variable containing the validation example pairs (In_spectr, label)
- Dataset_Params_log.json → Log file containing the dataset parameters

Fs → (INT) Sampling frequency to which the WAV files will be resampled.
Usage: If you have verified that the waveforms contain no energy above a certain frequency, you can resample to this frequency for faster processing.

seq_dur → (INT) Duration (in seconds) of the training sequence examples fed into the network.
Usage: Since the WAV files are long (e.g., 30 minutes), we split them into blocks of length seq_dur = 5 sec so that:
- Processing is faster
- We obtain more training examples

FE_params → (dictionary) Parameters of the FE (front end or input representation) used to feed the network.
We chose the FE to be the spectrogram so we can take advantage of CNN neural networks, as they work very well with images — and spectrograms are images.

classes_lookup → (dictionary) A lookup table where:
	- key → the name of the directory for a class (string)
	- value → the corresponding label (integer)

Important note: The key-value pair corresponding to the class with label = 0 must be written first in the classes_lookup dictionary! (Implementation requirement)                       
    
    
### 2) TRAIN
---------------------------------------------------------------------------------------------------------------------------------------
COMMAND:
python3 train.py --root /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst --nb_classes 4 --output /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/pretr_model --epochs 10

Parameter explaination: 
                
root → (STR) The path to the directory containing the files:
- Spec_seg_pair_list_train.pt
- Spec_seg_pair_list_valid.pt
- Dataset_Params_log.json
This directory is generated by the previous script (Create_Dataset.py). Therefore, this path must be the same as the Target_folder parameter of Create_Dataset.py.

output → (STR) The path to the directory (created if it does not already exist) where the following files will be saved:
- model.pth → Required if you want to use the model for inference or evaluation.
- model.json → Log file containing training details (e.g., training-validation losses, execution time, dataset parameters, arguments used in the train.py script).
- model.chkpnt → Required if you want to resume the training of an already trained model from where it stopped.
- nb_classes → The number of classes for which the neural network will be trained.

Basic training hyperparameters:

- epochs → (INT) The number of epochs for which the neural network will be trained.
- batch-size → (INT) The batch size used to feed the network (the number of examples provided before performing backpropagation).

Advantages of larger batch size:
- The training process will reliably find a local minimum.
- Faster processing, as the GPU is utilized more efficiently.

Disadvantage:
– Higher memory requirements.

There are other hyperparameters as well, but it is recommended to leave them at their default values.   

### 3) EVALUATION
--------------------------------------------------------------------------------------------------------------------------------------
COMMAND:
python3 evaluate.py --method-name TST --Model_dir /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/pretr_model --root_TEST_dir /home/mnanos/ML_pipeline_code_multiple_classes_v0/tst_dataset --evaldir /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/evaldir

Parameter explaination:
   method-name → (STR) The name of the method being evaluated — for example, you might want to compare different input feature extractors (FEs) or different network architectures.
This name will appear in the logs (you can also leave it unset).

model_dir → (STR) The path to the directory containing the necessary files for the pretrained model.
(It must be the same as the output argument of the train.py script.)

root_TEST_dir → (STR) The path to the directory containing the testing WAV files. It must have the structure shown above.

evaldir → (STR) The path to the directory (created if it does not already exist) where the following files will be saved:

Eval_Log.json → Contains the script parameters and evaluation metrics.

scores.pickle → Contains the evaluation metrics stored as a Python 3 variable. 


### 4) INFERENCE
---------------------------------------------------------------------------------------------------------------------------------------
COMMANDS:
python3 inference.py --Model_dir /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/pretr_model --input-wav /home/mnanos/ML_pipeline_code_multiple_classes_v0/tst_dataset/test/kinhsh_koble/κίνηση-κομπλε-3.wav  

#### Parameter explaination:
    model_dir → (STR) The path to the directory containing the necessary files for the pretrained model.
(It must be the same as the output argument of the train.py script.)

input-wav → (STR) The path to an audio file for which you want to get a prediction.


To continue the training, run the following command:
--------------------------------------------------- 
python3 train.py --model /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/pretr_model --checkpoint /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/pretr_model /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst --nb_classes 4 --output /home/mnanos/ML_pipeline_code_multiple_classes_v0/Spectrograms_tst/pretr_model --epochs 10 


Key Notes and Remarks: 
DATASET: 
	The dataset (WAV files) was created with the help of Audacity software. The following procedure was followed: 
First, it should be noted that one class, whose corresponding WAV file was 22 seconds long, was not taken into account. This is why we ended up with 4 classes.

For the remaining 4 classes, we trimmed the WAV files we had for each one so that they could be appropriately placed into the 3 folders (test, train, valid).
The durations of the corresponding WAV files follow the order: Train > Test > Valid.

The above procedure can easily be:
1. Extended to include more than one WAV file per class.
2. Extended to include more classes.
		    
Neural Network:
The network predicts, for a block of duration seq_dur = 5 sec, which class (out of the 4) it belongs to.
The network output is a matrix of dimensions (nb_blocks, nb_classes), where each row contains the unnormalized responses for the 4 classes for a single block.

The objective function we use for minimization is CrossEntropyLoss
https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html,
where we also apply class weights, since the dataset is unbalanced.

nb_blocks = number of blocks → signal_duration / seq_dur
nb_classes = 4 (types of sounds we want to recognize)
		     

Evaluation and Inference:
First, it should be emphasized that both inference and evaluation are performed in blocks of duration seq_dur, just as in training.

Evaluation:
In evaluation, we assess how well the network classifies the given blocks into their respective classes.
To determine which class a block belongs to:

We apply the softmax() non-linearity to the network’s unnormalized response (a 4-element vector).

This step transforms the unnormalized responses into 4 probabilities.

The index of the highest probability in this vector corresponds to the class predicted by the network for that block.

Inference:
In inference, to determine which class the entire input WAV belongs to, we apply the same step used in evaluation to each block’s output.
However, here we aim to decide the class of the entire WAV file, not just its individual blocks.
Thus, we select the class with the highest number of occurrences across all blocks—similar to analyzing a histogram.
(In other words, we can treat the predicted class of each block as a random variable, and this histogram effectively estimates the distribution of that variable.)
		    
